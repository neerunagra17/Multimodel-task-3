{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few sentiment labels:\n",
      "   overall_sentiment\n",
      "0                  4\n",
      "1                  4\n",
      "2                  3\n",
      "3                  3\n",
      "4                  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.2933\n",
      "Epoch 2/5, Loss: 1.2812\n",
      "Epoch 3/5, Loss: 1.2701\n",
      "Epoch 4/5, Loss: 1.2647\n",
      "Epoch 5/5, Loss: 1.2586\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, CLIPProcessor, CLIPModel, BertModel\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Allow PIL to load truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Data Preparation\n",
    "# ---------------------------\n",
    "\n",
    "csv_path = 'labels.csv'  # Update with your actual CSV file path\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ensure that text fields are strings (fill NaN with empty strings)/\n",
    "df['text_ocr'] = df['text_ocr'].fillna(\"\")\n",
    "\n",
    "# Define sentiment mapping (5 classes: 0 through 4)\n",
    "sentiment_mapping = {\n",
    "    'very_positive': 4,\n",
    "    'positive': 3,\n",
    "    'neutral': 2,\n",
    "    'negative': 1,\n",
    "    'very_negative': 0,\n",
    "}\n",
    "df['overall_sentiment'] = df['overall_sentiment'].map(sentiment_mapping)\n",
    "print(\"First few sentiment labels:\")\n",
    "print(df[['overall_sentiment']].head())\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Setup Tokenizers and Processors\n",
    "# ---------------------------\n",
    "\n",
    "# BERT tokenizer for text\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# CLIP processor and model for images\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()  # We use CLIP's image encoder in evaluation mode\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define the Dataset\n",
    "# ---------------------------\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, clip_processor, clip_model, image_folder, max_text_length=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.clip_processor = clip_processor\n",
    "        self.clip_model = clip_model\n",
    "        self.image_folder = image_folder\n",
    "        self.max_text_length = max_text_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- Process Text ---\n",
    "        text = row['text_ocr']\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_text_length\n",
    "        )\n",
    "        # Remove extra batch dimension for proper collation\n",
    "        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n",
    "        \n",
    "        # --- Process Image ---\n",
    "        image_path = os.path.join(self.image_folder, row['image_name'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_inputs = self.clip_processor(images=image, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.get_image_features(**image_inputs)\n",
    "        image_features = image_features.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # --- Process Label ---\n",
    "        label = torch.tensor(row['overall_sentiment']).long()\n",
    "        \n",
    "        return text_inputs, image_features, label\n",
    "\n",
    "# Set the folder where your images are stored\n",
    "image_folder = 'images'  # Update with your actual images folder path\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, bert_tokenizer, clip_processor, clip_model, image_folder)\n",
    "val_dataset   = MultimodalDataset(val_df, bert_tokenizer, clip_processor, clip_model, image_folder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define the Multimodal Model\n",
    "# ---------------------------\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, bert_model, image_dim, num_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.bert_model = bert_model  # Pretrained BERT model\n",
    "        # Project BERT's pooled output (typically 768) to 128 dimensions\n",
    "        self.text_fc = nn.Linear(bert_model.config.hidden_size, 128)\n",
    "        # Project CLIP image features (512-dim for clip-vit-base-patch32) to 128 dimensions\n",
    "        self.image_fc = nn.Linear(image_dim, 128)\n",
    "        # Combine both and predict sentiment (ensure num_classes=5 for labels 0â€“4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_inputs, image_features):\n",
    "        bert_outputs = self.bert_model(**text_inputs)\n",
    "        pooled_text = bert_outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        text_out = self.text_fc(pooled_text)\n",
    "        image_out = self.image_fc(image_features)\n",
    "        combined_features = torch.cat((text_out, image_out), dim=1)\n",
    "        logits = self.fc(combined_features)\n",
    "        return logits\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()  # Optionally set to eval mode if you don't want to fine-tune BERT\n",
    "\n",
    "# IMPORTANT: Set num_classes=5 to match the sentiment mapping!\n",
    "num_classes = 5\n",
    "model = MultimodalModel(bert_model=bert_model, image_dim=512, num_classes=num_classes)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training Setup\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Training Loop\n",
    "# ---------------------------\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        text_inputs, image_features, labels = batch\n",
    "        \n",
    "        # Move inputs to the device\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        image_features = image_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text_inputs, image_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
